{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(path):\n",
    "    \"\"\"Create path if not exist\"\"\"\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "\n",
    "def txt_to_string(text_file_path):\n",
    "    \"\"\"Read text file as string\"\"\"\n",
    "\n",
    "    f = open(text_file_path, 'r')\n",
    "    txt_string = ''\n",
    "    while True:\n",
    "        single_line = f.readline()\n",
    "        if single_line == '':\n",
    "            break\n",
    "        txt_string += single_line\n",
    "    f.close()\n",
    "\n",
    "    return txt_string\n",
    "\n",
    "\n",
    "def create_rnn_dataset(txt_array, max_chars):\n",
    "    \"\"\"\n",
    "    This function would return a training matrix and a label set\n",
    "    matrix: # rows represents number of sentence observations\n",
    "            # cols represents max number of chars in each sentence\n",
    "    \"\"\"\n",
    "    txt_array_label = txt_array[1:]\n",
    "    nrows = int(np.floor(txt_array.shape[0] / max_chars))\n",
    "    txt_array = txt_array[:nrows*max_chars]\n",
    "    txt_array_label = txt_array_label[:nrows*max_chars]\n",
    "    \n",
    "    txt_array = txt_array.reshape([-1, max_chars])\n",
    "    txt_array_label = txt_array_label.reshape([-1, max_chars])\n",
    "    \n",
    "    return txt_array, txt_array_label\n",
    "\n",
    "\n",
    "def train_val_split(train_mtx, label_mtx, train_proportion=0.8,\n",
    "                    random_state=666):\n",
    "    np.random.seed(random_state)\n",
    "    num_train_rows = np.round(train_mtx.shape[0] * train_proportion).astype(int)\n",
    "    rows_selected = np.random.choice(train_mtx.shape[0],\n",
    "                                     num_train_rows, replace=False)\n",
    "    rows_not_selected = list(\n",
    "        set(range(train_mtx.shape[0])) - set(rows_selected))\n",
    "    \n",
    "    return (train_mtx[rows_selected], train_mtx[rows_not_selected],\n",
    "            label_mtx[rows_selected], label_mtx[rows_not_selected])\n",
    "\n",
    "\n",
    "class RNNDataset():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.copy()\n",
    "        self.y = y.copy()\n",
    "        \n",
    "\n",
    "class BatchManager():\n",
    "\n",
    "    def __init__(self, train_set, num_epochs, shuffle=True,\n",
    "                 random_state=666):\n",
    "        \"\"\"\n",
    "        train_set, val_set: RNNDataset instances\n",
    "        \"\"\"\n",
    "        self.train_set = train_set\n",
    "        self.num_epochs = num_epochs\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.current_epoch = 0\n",
    "        self.rows_in_batch = []\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Output next batch as (X, y), return None if ran over num_epochs\n",
    "        \"\"\"\n",
    "        num_rows = self.train_set.X.shape[0]\n",
    "\n",
    "        while len(self.rows_in_batch) < batch_size:\n",
    "            self.current_epoch += 1\n",
    "            row_nums = list(range(num_rows))\n",
    "            if self.shuffle:\n",
    "                np.random.seed(self.random_state)\n",
    "                np.random.shuffle(row_nums)\n",
    "            self.rows_in_batch += row_nums\n",
    "            \n",
    "        selected_X = self.train_set.X[self.rows_in_batch[:batch_size]]\n",
    "        selected_y = self.train_set.y[self.rows_in_batch[:batch_size]]\n",
    "        self.rows_in_batch = self.rows_in_batch[batch_size:]\n",
    "\n",
    "        if self.current_epoch > self.num_epochs:\n",
    "            return None\n",
    "        return selected_X, selected_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chars = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text file as string\n",
    "txt_string = txt_to_string('./data/shakespeare.txt')\n",
    "\n",
    "# convert characters to numbers\n",
    "txt_char_ls = list(txt_string)\n",
    "unique_chars = np.unique(txt_char_ls)\n",
    "le_char = LabelEncoder()\n",
    "le_char.fit(unique_chars)\n",
    "txt_num_ls = le_char.transform(txt_char_ls)\n",
    "txt_num_array = np.array(txt_num_ls)\n",
    "\n",
    "# construct text dataset matrix and label set\n",
    "txt_mtx, txt_mtx_label = create_rnn_dataset(txt_num_array, max_chars)\n",
    "\n",
    "# create train test split\n",
    "train_x, val_x, train_y, val_y = train_val_split(\n",
    "    txt_mtx, txt_mtx_label, train_proportion=0.8, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(unique_chars)\n",
    "EMBEDDING_SIZE = 100\n",
    "NUM_RNN_LAYER_UNITS = [128, 128]\n",
    "KEEP_PROB = 0.8\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 500\n",
    "NUM_EPOCHS = 200\n",
    "SHUFFLE_BATCH = True\n",
    "EVAL_FREQUENCY = 10\n",
    "EARLY_STOPPING_EVAL_ROUNDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "time_now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "tf_graph_dir = './tf_graph/run-{}/'.format(time_now)\n",
    "tf_model_dir = './tf_model/model-{}/'.format(time_now)\n",
    "\n",
    "create_path(tf_model_dir)\n",
    "with open(tf_model_dir+'char_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le_char, f)\n",
    "\n",
    "with graph.as_default():\n",
    "    txt_input = tf.placeholder(tf.int32, [None, None], 'text_input')\n",
    "    txt_input_next = tf.placeholder(tf.int32, [None, None], 'text_label')\n",
    "    txt_input_next_onehot = tf.one_hot(txt_input_next, depth=VOCAB_SIZE,\n",
    "                                       axis=2, dtype=tf.int32, name='text_label_onehot')\n",
    "\n",
    "    with tf.variable_scope('embedding'):\n",
    "        embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBEDDING_SIZE]))    \n",
    "        txt_embedded = tf.nn.embedding_lookup(embed_matrix, txt_input)\n",
    "\n",
    "    with tf.variable_scope('rnn_1'):\n",
    "        lstm_1 = tf.nn.rnn_cell.BasicLSTMCell(NUM_RNN_LAYER_UNITS[0], activation=tf.nn.tanh)\n",
    "        lstm_dropout_1 = tf.nn.rnn_cell.DropoutWrapper(lstm_1, output_keep_prob=KEEP_PROB)\n",
    "        rnn_1 = tf.nn.dynamic_rnn(lstm_dropout_1, txt_embedded, dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('rnn_2'):\n",
    "        lstm_2 = tf.nn.rnn_cell.BasicLSTMCell(NUM_RNN_LAYER_UNITS[1], activation=tf.nn.tanh)\n",
    "        lstm_dropout_2 = tf.nn.rnn_cell.DropoutWrapper(lstm_2, output_keep_prob=KEEP_PROB)\n",
    "        rnn_2 = tf.nn.dynamic_rnn(lstm_dropout_2, rnn_1[0], dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('concat'):\n",
    "        concat_out = tf.concat([txt_embedded, rnn_1[0], rnn_2[0]], axis=2)\n",
    "        \n",
    "    with tf.variable_scope('output'):\n",
    "        logit_out = tf.layers.dense(concat_out, VOCAB_SIZE)\n",
    "        softmax_out = tf.nn.softmax(logit_out)\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        loss_word = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=txt_input_next_onehot, logits=logit_out)\n",
    "        loss_sum_sentence = tf.reduce_sum(loss_word, axis=1)\n",
    "        loss_avg_batch = tf.reduce_mean(loss_sum_sentence)\n",
    "        \n",
    "    with tf.variable_scope('optimization'):\n",
    "        optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "        train = optimizer.minimize(loss_avg_batch)\n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "# tf.summary.FileWriter(tf_graph_dir, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_score = 99999\n",
    "step_counter = 1\n",
    "early_stopping_counter = 0\n",
    "train_set = RNNDataset(train_x, train_y)\n",
    "val_set = RNNDataset(val_x, val_y)\n",
    "batch_manager = BatchManager(train_set, num_epochs=NUM_EPOCHS,\n",
    "                             shuffle=SHUFFLE_BATCH, random_state=666)\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        while True:\n",
    "            batch = batch_manager.next_batch(BATCH_SIZE)\n",
    "            if batch is None:\n",
    "                break\n",
    "            batch_x, batch_y = batch[0], batch[1]\n",
    "\n",
    "            if step_counter % EVAL_FREQUENCY == 0:\n",
    "                train_loss = sess.run(loss_avg_batch, feed_dict={\n",
    "                    txt_input:batch_x,\n",
    "                    txt_input_next:batch_y\n",
    "                })\n",
    "\n",
    "                val_loss = sess.run(loss_avg_batch, feed_dict={\n",
    "                    txt_input:val_set.X,\n",
    "                    txt_input_next:val_set.y\n",
    "                })\n",
    "                \n",
    "                print('Training Loss: {} | Validation Loss: {}'.format(\n",
    "                    train_loss, val_loss))\n",
    "\n",
    "                if val_loss < bst_score:\n",
    "                    early_stopping_counter = 0\n",
    "                    saver.save(sess, tf_model_dir+'char_generator.ckpt')\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "                    \n",
    "                if early_stopping_counter > EARLY_STOPPING_EVAL_ROUNDS:\n",
    "                    break\n",
    "\n",
    "            sess.run(train, feed_dict={\n",
    "                txt_input:batch_x,\n",
    "                txt_input_next:batch_y\n",
    "            })\n",
    "\n",
    "            step_counter += 1\n",
    "\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
